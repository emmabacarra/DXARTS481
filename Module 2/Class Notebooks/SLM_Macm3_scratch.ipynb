{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "729f73d3-8c31-4997-bea2-dd410ff67b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers version: 4.48.2\n",
      "Datasets version: 3.2.0\n",
      "Torch version: 2.6.0\n",
      "MPS Available: True\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import datasets\n",
    "import torch\n",
    "import os #to do system stuff, like making folders\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from accelerate import Accelerator\n",
    "\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "\n",
    "print(f\"Datasets version: {datasets.__version__}\")\n",
    "print(f\"Torch version: {torch.__version__}\")\n",
    "print(f\"MPS Available: {torch.backends.mps.is_available()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e17ea4b2-91bd-4787-9c94-c79bf99a4ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created folder: corpus\n"
     ]
    }
   ],
   "source": [
    "# Make a folder, to hold .txt corpus\n",
    "## only run this block one once! \n",
    "folder1 = \"corpus\"\n",
    "# Create the folder in the current directory\n",
    "os.makedirs(folder1, exist_ok=True)\n",
    "print(f\"Created folder: {folder1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16469aa1-0d51-450b-b223-f2d3719d91cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to use tinkyshakespeare text run this cell\n",
    "import requests\n",
    "\n",
    "# Download the Tiny Shakespeare dataset\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "response = requests.get(url)\n",
    "data = response.text\n",
    "\n",
    "# Save to file\n",
    "with open('corpus/tiny_shakespeare.txt', 'w') as f:\n",
    "    f.write(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5a37af6a-23b8-4727-8023-e4f6faf3b717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text data read from file and saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# if you want to use your own .txt files run this cell\n",
    "\n",
    "# Define the path to your own text file\n",
    "corpus = 'corpus/5Pages_roomsdescriptions.txt'  # Replace with the actual path to your text file\n",
    "\n",
    "# Read the text file\n",
    "with open(corpus, 'r') as file:\n",
    "    data = file.read()\n",
    "\n",
    "# Save to file (if needed)\n",
    "output_txt = 'corpus/roomsD.txt'  # Replace with your desired output file path\n",
    "with open(corpus, 'w') as f:\n",
    "    f.write(data)\n",
    "\n",
    "print(\"Text data read from file and saved successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e52c9839-4b18-4264-a57b-1a6cef60246b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description for posting 3:\n",
      "I'm looking for a roommate to share a 2b/2ba 1300 sqft condo near downtown Redmond. Pet rent is waived for cats!\n",
      "Posted pictures are old. The interior furnishings look different now, but the walls/flooring are the same.\n",
      "- Bedroom is 11'x12' and with sizable closet. Rent is $1600 a month (Lease would be up April 2026 with potential to renew)\n",
      "- Rent includes water/sewer/garbage/wifi/renter's insurance\n",
      "- 1 free parking spot for you\n",
      "- 15 minute walk from Redmond Town Center, 7 minutes to a 545 stop, close to the 520.\n",
      "- Very close to Microsoft/Google/Facebook\n",
      "- Community hot tub and pool\n",
      "- Recently renovated with wood floors, all new kitchen appliances, washer/dryer in unit, balcony\n",
      "- I'm the only other person that will be living in the unit\n",
      "- place is furnished, but \n"
     ]
    }
   ],
   "source": [
    "# lets make sure our corpus was loaded and read successfully:\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load data into a Hugging Face Dataset\n",
    "raw_data = Dataset.from_dict({'text': [data]})\n",
    "\n",
    "# Print the first 500 characters\n",
    "print(raw_data['text'][0][:800])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ba9c4eb9-c174-42be-aaa9-cb03fa733610",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "430724f7-cff9-4e56-a175-382019df602f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (67891 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  4.81 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], return_special_tokens_mask=True)\n",
    "\n",
    "tokenized_dataset = raw_data.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    remove_columns=[\"text\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4e783615-510f-4bd8-b3e5-f899459044fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 17.71 examples/s]\n"
     ]
    }
   ],
   "source": [
    "block_size = 128\n",
    "\n",
    "def group_texts(examples):\n",
    "    concatenated = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated['input_ids'])\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    result = {\n",
    "        k: [concatenated[k][i:i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k in concatenated.keys()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "lm_dataset = tokenized_dataset.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "eb18b5a7-23dc-450d-9f64-de05f47919d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 15.88 examples/s]\n"
     ]
    }
   ],
   "source": [
    "block_size = 128\n",
    "\n",
    "def group_texts(examples):\n",
    "    concatenated = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated['input_ids'])\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    result = {\n",
    "        k: [concatenated[k][i:i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k in concatenated.keys()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "lm_dataset = tokenized_dataset.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4646e21b-ad07-4451-a4b0-3adce2709312",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 530/530 [00:00<00:00, 27281.53 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Save the dataset to disk\n",
    "lm_dataset.save_to_disk('lm_dataset')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2456e401-312f-472e-b133-6988e872325e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the GPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "95cd75e4-d31e-4d79-828d-dfef76ab8102",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "\n",
    "# Load the dataset\n",
    "lm_dataset = load_from_disk('lm_dataset')\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1d5a7754-6474-4b0d-a045-302c8770c51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS backend\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS backend\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b45ea5c5-e49c-42bc-a873-e70ff9e484ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/NLP/lib/python3.11/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./model_weights',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=4,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=100,\n",
    "    save_steps=500,\n",
    "    logging_steps=50,\n",
    "    learning_rate=5e-4,\n",
    "    warmup_steps=100,\n",
    "    save_total_limit=2,\n",
    "    fp16=False,  # MPS backend currently doesn't support fp16 as of writing.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "17703e95-07a3-4fda-a0dd-55fd347da1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# Data collator for dynamic padding\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_dataset,\n",
    "    eval_dataset=lm_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1742e3bf-8f51-4bf7-9931-a280e54296d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a694fc95-80e0-4662-b13a-f284d7dd631d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1060' max='1060' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1060/1060 05:06, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.386600</td>\n",
       "      <td>0.300419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>0.431182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.513700</td>\n",
       "      <td>0.391328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.444700</td>\n",
       "      <td>0.331633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.441900</td>\n",
       "      <td>0.248327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.218900</td>\n",
       "      <td>0.196137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.232300</td>\n",
       "      <td>0.159839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.191700</td>\n",
       "      <td>0.104587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.086019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.110700</td>\n",
       "      <td>0.063547</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Training complete. Saving model...\n",
      "✅ Model saved.\n"
     ]
    }
   ],
   "source": [
    "### print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"✅ Training complete. Saving model...\")\n",
    "trainer.save_model('./myLM_gpt2')\n",
    "tokenizer.save_pretrained('./myLM')\n",
    "print(\"✅ Model saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "169b8cf8-44b9-4f19-8c9a-0a8b62dbf14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS backend\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a prompt (or type 'exit' to quit):  this is the home\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: this is the home\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/NLP/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:676: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oasis), Quiet spaces where everything is 420 friendly, We are really just looking for friendly roommates that are friendly and clean. We have a lot of longevity with our housemates and housemates, so a home of sorts is not available on the weekends by night.\n",
      "BEDROOM OF LIFE: 6 bedrooms, three 4 bath, washer bathroom, kitchen and dining area.\n",
      "NOTE: This is a no smoking house, no drugs home, no parties or\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a prompt (or type 'exit' to quit):  friendly and clean\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: friendly and clean\n",
      "\n",
      "- calm and reflective\n",
      "- responsible and not show animals\n",
      "- non-smoking \n",
      "- no recreational drugs \n",
      "- serious about clean relationships\n",
      "- employed professionals say plenty of service work in the animal care industry\n",
      "5) Abundant street parking\n",
      "6) Double parked cars\n",
      "7) Walking distance to Highland Sammamish Plateau, Phinney. There is free street parking and a covered deck for professional motorcycles, motorcycles, and schnbaskets\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a prompt (or type 'exit' to quit):  motorcycles, and schnbaskets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: motorcycles, and schnbaskets\n",
      ". Lots of kitchen space is available on 3rd and staining the floor of the house. For all 4 bedrooms: 6 beds, 1 shower, 1 frame, foodbasket, dishwasher. Microwave, refrigerator and oven. Garage work from home so cleaning up after yourself before you use any dishes in the house.\n",
      "One bedroom on the main floor (shares 1 wall of closet). The living room and bathrooms are fully equipped,\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a prompt (or type 'exit' to quit):   The living room and bathrooms are fully equipped\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:  The living room and bathrooms are fully equipped\n",
      " so please donUse of any washer/dryer in the house unless discussed with other housemates. Must be able to pass a background check.\n",
      "Location: Small neighborhood with very well lit backyard (2 miles) close to Lake City College Total 4405,5453,66 MSFT, PLU, South Lake Union, Ravenna, Seattle & Evergreen Redmond hospitals, Spanaway park, miles of trails, water, sewer, garbage, floors\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a prompt (or type 'exit' to quit):  Spanaway park, miles of trails, water, sewer, garbage, floors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Spanaway park, miles of trails, water, sewer, garbage, floors\n",
      ", heat, insulation, triple-pane windows, siding, bathrooms, kitchen with quartz countertop and pretty much everything except the foundation and studs / structure)\n",
      "*\n",
      "Includes brand new high-end appliances (Washer/Dryer, dishwasher, double-oven gas stove with overhead convection microwave/oven, french-door refridgerator) + other energy efficient upgrades (unlimited\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a prompt (or type 'exit' to quit):  refridgerator\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: refridgerator\n",
      ") that will make you happy to find your new partner.\n",
      "If you're interested in talking with me, please respond to this email with your last employment status and intended moving date as well as your contact info and possibly frame for your next roommate. If this sounds like it could be a good fit for you, please respond to this email with your email address.\n",
      "Thanks!\n",
      "\n",
      "Description for posting 224:\n",
      "I am looking for a new roommate to share my 2 bedroom\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a prompt (or type 'exit' to quit):  share my 2 bedrooms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: share my 2 bedrooms\n",
      ". Looking for someone who is clean and enjoys a peaceful atmosphere. Rent is 1100 per month and I'm flexible when moving out, I have a way to go higher when I need it, going on schedule time.\n",
      "Other Amenities:\n",
      "• Immediate access to i405 and its own private light rail/several transit stations\n",
      "• 5 min drive/several bus lines\n",
      "• Washer/dryer on site \n",
      "• Small street parking as well\n",
      "・\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a prompt (or type 'exit' to quit):  Rent is 1100 per month\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Rent is 1100 per month\n",
      " (month-to-month lease)  However, utilities can be paid at installment if needed. Rental references: A required background check (LOCATION: Seattle, City of Seattle, Shoreline, Issaquah) and a $500 rent deposit (equal to 1 month rent) \n",
      "Note: the above information, no smoking, drug, no pet, no  couples. \n",
      "Please contact agent Lisa at    \n",
      "show contact info\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a prompt (or type 'exit' to quit):  Please contact agent Lisa\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Please contact agent Lisa\n",
      " at \n",
      "show contact info\n",
      "\n",
      "Description for posting 128:\n",
      "ASE STOP FLAGGING THIS POST! IT IS LEGAL TO REVE A LIVING ON PHONE WITH A HARD W FOR ONE TENANT-  PERSON WITH A HARDWOOD FLAGGED BATHROOM.\n",
      "-VE IN REQUIREMENT:  (1) must pay deposit, 1st month's rent, (Last month's rent)\n",
      "1.3\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a prompt (or type 'exit' to quit):  (1) must pay deposit, 1st month's rent, (Last month's rent)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: (1) must pay deposit, 1st month's rent, (Last month's rent)\n",
      " refundable. Move in may be available within 2 months of move-in, depending on tenant credit and employment.\n",
      "Walk-score=90: This is one of the most walkable neighborhoods in Seattle. The walk score of most of the other locations in CL is only 30-60. This is a dream location for working people. Everything is nearby so most errands are accomplished while completing the search\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a prompt (or type 'exit' to quit):  most errands are accomplished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: most errands are accomplished\n",
      "). \n",
      "One room will be available on the first of January 15th:\n",
      "Private bathroom for extra privacy. Share the kitchen and laundry with one other person. \n",
      "No additional pets. \n",
      "Parking on the property. \n",
      "Off street parking with mature trees\n",
      "Fenced back garden\n",
      "New appliances\n",
      "Owner occupied only\n",
      "Share utilities WIFI\n",
      "Off street parking for one car per station\n",
      "Credit background check\n",
      "5 minute walk to Safeway on the\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a prompt (or type 'exit' to quit):  Parking on the property.  Off street parking with mature trees Fenced back garden New appliances Owner occupied only Share utilities WIFI Off street parking for one car per station Credit background check 5 minute walk to Safeway on the\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Parking on the property.  Off street parking with mature trees Fenced back garden New appliances Owner occupied only Share utilities WIFI Off street parking for one car per station Credit background check 5 minute walk to Safeway on the\n",
      " property\n",
      "Owner occupied only by his partner\n",
      "Monthly rental includes all utilities, 80 SQFT\n",
      "Private bath, 75 SQFT\n",
      "Lawnwalk to Children's Hospital,ARD,MCA,N.V. , and a covered deck forooterOCOL.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a prompt (or type 'exit' to quit):  covered deck forooterOCOL.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: covered deck forooterOCOL.\n",
      "  We have double kayaks, a fire pit, a large off-street parking space with an off-street parking option. Conveniently located near the intersection to North Tacoma PCC, Tacoma General Hospital, Kent Tukwila, and the Ashway Park Road to downtown Seattle.\n",
      "The town is filled with beautiful natural light and a muted south-facing porches. Large, fenced-equiped kitchen, dining room are all brand new,\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a prompt (or type 'exit' to quit):  The town is filled with beautiful natural light and a muted south-facing porches. Large, fenced-equiped kitchen, dining room are all brand new,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: The town is filled with beautiful natural light and a muted south-facing porches. Large, fenced-equiped kitchen, dining room are all brand new,\n",
      " available now. Laundry in unit, kitchen and bath are all up-and-street; dishes are available for use.\n",
      "The vibe is somewhere between quirky and punk, with lots of writing and art on the walls. There's even a street art listing on the wall that features a little more furniture than a regular refrigerator,\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a prompt (or type 'exit' to quit):  The vibe is somewhere between quirky and punk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: The vibe is somewhere between quirky and punk\n",
      ", with lots of writing and art on the walls.\n",
      "There are bookshelves, pans, trash bin, pots, pans and a communal deck with some communal storage.\n",
      "There is an also a private deck with some yardwork on the patio that could make some interesting reading for an inexpensive fee. There is an also a gal that is a super-friendly pit bull named Salem.\n",
      "What we are looking for: We all enjoy paddling, biking\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a prompt (or type 'exit' to quit):  exit\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('./myLM')\n",
    "model = GPT2LMHeadModel.from_pretrained('./myLM_gpt2')\n",
    "\n",
    "# Define the pad_token as eos_token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Update the model's configuration to recognize the pad_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Configure device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS backend\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "def generate_text(prompt, max_length=100):\n",
    "    # Encode the prompt and generate attention_mask\n",
    "    encoded = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors='pt',\n",
    "        padding=False,  # No padding needed for single inputs\n",
    "        truncation=False,\n",
    "        max_length=500  # Adjust based on model's max input length\n",
    "    )\n",
    "    input_ids = encoded['input_ids'].to(device)\n",
    "    attention_mask = encoded['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,  # Pass the attention_mask\n",
    "            max_length=max_length,\n",
    "            num_beams=1,\n",
    "            no_repeat_ngram_size=0, #Minimum value is 0 (no restriction). Higher values prevent repetitions of n-grams of that size\n",
    "            early_stopping=True,\n",
    "            temperature=1.8, #between 0.1 and 2.0 = higher = more gibberish\n",
    "            top_p=1.0, #Between 0.0 and 1.0. Lower values restrict the model \\ higher values allow for more diversity\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id  # Ensure pad_token_id is set\n",
    "        )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=False)\n",
    "\n",
    "def main():\n",
    "    while True:\n",
    "        prompt = input(\"Enter a prompt (or type 'exit' to quit): \")\n",
    "        if prompt.lower() == 'exit':\n",
    "            break\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        print(generate_text(prompt)[len(prompt):], sep=\"\")\n",
    "        print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if \"ipykernel_launcher\" in sys.argv[0]:\n",
    "        main()\n",
    "    else:\n",
    "        # Running as a standalone script\n",
    "        parser = argparse.ArgumentParser(description=\"Generate text based on a prompt.\")\n",
    "        parser.add_argument(\"--prompt\", nargs=\"?\", default=None, help=\"The prompt to generate text from.\")\n",
    "        args = parser.parse_args()\n",
    "        main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a9d2ef20-0aa1-40c1-bbf0-a399b3de954a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS backend\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a prompt:  there is a siwming ppool with ample street parking by the shore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response 1 appended to responses/responses1.txt\n",
      "Response 2 appended to responses/responses1.txt\n",
      "Response 3 appended to responses/responses1.txt\n",
      "Response 4 appended to responses/responses1.txt\n",
      "Response 5 appended to responses/responses1.txt\n",
      "Response 6 appended to responses/responses1.txt\n",
      "Response 7 appended to responses/responses1.txt\n",
      "Response 8 appended to responses/responses1.txt\n",
      "Response 9 appended to responses/responses1.txt\n",
      "Response 10 appended to responses/responses1.txt\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('./myLM')\n",
    "model = GPT2LMHeadModel.from_pretrained('./myLM_gpt2')\n",
    "\n",
    "# Define the pad_token as eos_token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Update the model's configuration to recognize the pad_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Configure device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS backend\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "def generate_text(prompt, max_length=100, num_responses=10):\n",
    "    responses = []\n",
    "    encoded = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors='pt',\n",
    "        padding=False,\n",
    "        truncation=False,\n",
    "        max_length=500\n",
    "    )\n",
    "    input_ids = encoded['input_ids'].to(device)\n",
    "    attention_mask = encoded['attention_mask'].to(device)\n",
    "\n",
    "    for _ in range(num_responses):\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=max_length,\n",
    "                num_beams=1,\n",
    "                no_repeat_ngram_size=0,\n",
    "                early_stopping=True,\n",
    "                temperature=1.8,\n",
    "                top_p=1.0,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.pad_token_id\n",
    "            )\n",
    "        responses.append(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "    \n",
    "    return responses\n",
    "\n",
    "def main():\n",
    "    prompt = input(\"Enter a prompt: \")\n",
    "    num_responses = 10  # Number of responses to generate\n",
    "    responses = generate_text(prompt, num_responses=num_responses)\n",
    "    \n",
    "    # Specify the output file name\n",
    "    output_file = 'responses/responses1.txt'\n",
    "    \n",
    "    # Append each response to .txt file\n",
    "    with open(output_file, 'a') as file:\n",
    "        for i, response in enumerate(responses):\n",
    "            file.write(f\"Response {i+1}:\\n{response}\\n\\n\")\n",
    "            print(f\"Response {i+1} appended to {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
