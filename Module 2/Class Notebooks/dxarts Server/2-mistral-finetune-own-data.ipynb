{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "VC-9m2yv3m18"
   },
   "source": [
    "### 0. Preparing data\n",
    "`.jsonl` files structured like this:\n",
    "```\n",
    "{\"input\": \"What color is the sky?\", \"output\": \"The sky is blue.\"}\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "E2CkxsA43m15"
   },
   "source": [
    "### 1. Instantiate GPU & Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "FuXIFTFapAMI",
    "outputId": "c8ced1ad-c7b3-44ba-807b-26d7d13906bc"
   },
   "outputs": [],
   "source": [
    "# # You only need to run this once per machine\n",
    "# !pip install -q -U bitsandbytes\n",
    "# !pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "# !pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "# !pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "# !pip install -q -U datasets scipy ipywidgets matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "s6f4z8EYmcJ6"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset('json', data_files='notes.jsonl', split='train')\n",
    "eval_dataset = load_dataset('json', data_files='notes_validation.jsonl', split='train')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "-9KNTJZkyRgn"
   },
   "source": [
    "\n",
    "Let's use Weights & Biases to track our training metrics. You'll need to apply an API key when prompted. Feel free to skip this if you'd like, and just comment out the `wandb` parameters in the `Trainer` definition below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "DDqUNyIoyRgo"
   },
   "outputs": [],
   "source": [
    "# !pip install -q wandb -U\n",
    "\n",
    "# import wandb, os\n",
    "# wandb.login()\n",
    "\n",
    "# wandb_project = \"journal-finetune\"\n",
    "# if len(wandb_project) > 0:\n",
    "#     os.environ[\"WANDB_PROJECT\"] = wandb_project"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "uhw8JiOr3m18"
   },
   "source": [
    "### Formatting prompts\n",
    "Then create a `formatting_func` to structure training examples as prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "f-fJR0MlQiTD"
   },
   "outputs": [],
   "source": [
    "def formatting_func(example):\n",
    "    text = f\"### Previous: {example['input']}\\n ### Next: {example['output']}\"\n",
    "    return text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "sflV0DL2P64_"
   },
   "source": [
    "Common ones:\n",
    "\n",
    "```python\n",
    "def formatting_func(example):\n",
    "    text = f\"### Question: {example['input']}\\n ### Answer: {example['output']}\"\n",
    "    return text\n",
    "```\n",
    "```python\n",
    "def formatting_func(example):\n",
    "    text = f\"### The following is a note by Eevee the Dog: {example['note']}\"\n",
    "    return text\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "shz8Xdv-yRgf"
   },
   "source": [
    "### 2. Load Base Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "MJ-5idQwzvg-"
   },
   "source": [
    "Let's now load Mistral - mistralai/Mistral-7B-v0.1 - using 4-bit quantization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "The token `mistralai/Mistral-7B-v0.1` has been saved to /root/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful.\n",
      "The current active token is: `mistralai/Mistral-7B-v0.1`\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "!huggingface-cli login --token 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "45524c98039a46d5b7745ad7cb638d2f"
     ]
    },
    "id": "E0Nl5mWL0k2T",
    "outputId": "47b6b01d-e9f2-4b70-919c-17ae64993843"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-16 22:59:39.720184: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1737068379.739912    1414 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1737068379.746551    1414 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-16 22:59:39.768376: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ca7f042696c4a50bb07ba3b55b0c954",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5833c2a604648cca16dca29f8e4e9d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d104d2879f604650b6f7bed93c5ec89e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05089a76fe1b4bff815af05d3af6e4a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31001f21bffb4956846bfb6d180e81d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d376e20bf474aecb4b831a8db9f3665",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "base_model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config, device_map=\"auto\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "UjNdXolqyRgf"
   },
   "source": [
    "### 3. Tokenization\n",
    "\n",
    "Set up the tokenizer. Add padding on the left as it [makes training use less memory](https://ai.stackexchange.com/questions/41485/while-fine-tuning-a-decoder-only-llm-like-llama-on-chat-dataset-what-kind-of-pa).\n",
    "\n",
    "\n",
    "For `model_max_length`, it's helpful to get a distribution of your data lengths. Let's first tokenize without the truncation/padding, so we can get a length distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install tiktoken blobfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "haSUDD9HyRgf",
    "outputId": "22ee95db-2974-4ab0-e0c7-444d04d3e838"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True,\n",
    "    add_bos_token=True,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def generate_and_tokenize_prompt(prompt):\n",
    "    return tokenizer(formatting_func(prompt))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "WHnKLcq4yRgg"
   },
   "source": [
    "Reformat the prompt and tokenize each sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "S3iLAwLh3m19"
   },
   "outputs": [],
   "source": [
    "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt)\n",
    "tokenized_val_dataset = eval_dataset.map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "O6ewk27p3m19"
   },
   "source": [
    "Let's get a distribution of our dataset lengths, so we can determine the appropriate `max_length` for our input tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "BA8M9yfC3m19",
    "outputId": "99c6d302-9bb6-47b1-cae9-a1cd870b4770"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7446\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset):\n",
    "    lengths = [len(x['input_ids']) for x in tokenized_train_dataset]\n",
    "    lengths += [len(x['input_ids']) for x in tokenized_val_dataset]\n",
    "    print(len(lengths))\n",
    "\n",
    "    # Plotting the histogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(lengths, bins=20, alpha=0.7, color='blue')\n",
    "    plt.xlabel('Length of input_ids')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Lengths of input_ids')\n",
    "    plt.show()\n",
    "\n",
    "plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "nBk4Qp_vyRgh"
   },
   "source": [
    "From here, you can choose where you'd like to set the `max_length` to be. You can truncate and pad training examples to fit them to your chosen size. Be aware that choosing a larger `max_length` has its compute tradeoffs.\n",
    "\n",
    "I'm using my personal notes to train the model, and they vary greatly in length. I spent some time cleaning the dataset so the samples were about the same length, cutting up individual notes if needed, but being sure to not cut in the middle of a word or sentence."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "bMlw8h743m19"
   },
   "source": [
    "Now let's tokenize again with padding and truncation, and set up the tokenize function to make labels and input_ids the same. This is basically what [self-supervised fine-tuning is](https://neptune.ai/blog/self-supervised-learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "acINaViR3m19"
   },
   "outputs": [],
   "source": [
    "max_length = 512 # appropriate max length for your dataset\n",
    "\n",
    "def generate_and_tokenize_prompt2(prompt):\n",
    "    result = tokenizer(\n",
    "        formatting_func(prompt),\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "518d4f0b89bf4d57bf00d4c6d6e59eb5"
     ]
    },
    "id": "lTk-aTog3m19",
    "outputId": "4fb637b4-77a2-47c6-de7b-4fb620663dd7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "987a89e5999741e79f410e480ec22442",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1490 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt2)\n",
    "tokenized_val_dataset = eval_dataset.map(generate_and_tokenize_prompt2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "TQL796OayRgh"
   },
   "source": [
    "Check that `input_ids` is padded on the left with the `eos_token` (2) and there is an `eos_token` 2 added to the end, and the prompt starts with a `bos_token` (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "OKHhvxK83m19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 774, 4258, 2123, 28747, 985, 28717, 699, 286, 28725, 19183, 559, 6906, 28725, 304, 5591, 28723, 13, 774, 8580, 28747, 650, 2580, 272, 4338, 302, 559, 28720, 17494, 28725, 272, 3575, 302, 559, 23623, 297, 16080, 369, 682, 295, 3410, 27846, 823, 272, 1080, 3459, 288, 9890, 302, 2970, 3126, 846, 1063, 28723, 2]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_train_dataset[1]['input_ids'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "I6LRa2Zm3m19"
   },
   "source": [
    "Now all the samples should be the same length, `max_length`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "I55Yo3yy3m19",
    "outputId": "c87e344d-e0f3-4542-afcc-4e2025926d64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7446\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "jP3R4enP3m19"
   },
   "source": [
    "### How does the base model do?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Vxbl4ACsyRgi"
   },
   "source": [
    "Optionally, you can check how Mistral does on one of your data samples. For example, if you have a dataset of users' biometric data to their health scores, you could test the following `eval_prompt`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "gOxnx-cAyRgi"
   },
   "outputs": [],
   "source": [
    "eval_prompt = \"\"\" Given the following biometric data, score the users' health, from 0-100.\n",
    "\n",
    "### Biometric Data:\n",
    "Temperature=98.2,\n",
    "Sex=F,\n",
    "Age=29,\n",
    "Height=69 inches,\n",
    "Weight=160 lbs,\n",
    "V02_Max=55,\n",
    "HRV=55\n",
    "\n",
    "### Health Score:\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "KRhfq_Fa3m19"
   },
   "source": [
    "The `eval_prompt` I used was:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "pa6ux9ni3m19"
   },
   "outputs": [],
   "source": [
    "eval_prompt = \" The following is a test: # \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "NidIuFXMyRgi",
    "outputId": "b1794b11-9a22-4b0a-e871-7df039ab59fc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a test: # 1. What do you call the person who writes the words to a song? A lyricist B. Composer C. Musician D. Singer E. All of the above\n",
      "\n",
      "# 2. Which of these instruments are not used in jazz music? A. Piano B. Saxophone C. Trumpet D. Drums E. Violin\n",
      "\n",
      "# 3. Who was the first African American woman to win an Oscar for Best Original Song? A. Halle Berry B. Jennifer Hudson C. Regina King D. Lena Horne E. Mary J Blige\n",
      "\n",
      "# 4. Which of these songs were written by Stevie Wonder? A. I Just Called To Say I Love You B. Superstition C. My Cherie Amour D. For Once In My Life E. All Of The Above\n",
      "\n",
      "# 5. Which of these artists did NOT write their own lyrics? A. Beyonce B. Jay Z C. Kanye West D. Lauryn Hill E. Missy Elliott\n",
      "\n",
      "Answers: 1. E, 2. E, 3. D, 4. E, 5. B\n"
     ]
    }
   ],
   "source": [
    "# Init an eval tokenizer that doesn't add padding or eos token\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    add_bos_token=True,\n",
    ")\n",
    "\n",
    "model_input = eval_tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(eval_tokenizer.decode(model.generate(**model_input, max_new_tokens=256, repetition_penalty=1.15)[0], skip_special_tokens=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "dCAWeCzZyRgi"
   },
   "source": [
    "Observe how the model does out of the box."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "AapDoyfAyRgi"
   },
   "source": [
    "### 4. Set Up LoRA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Mp2gMi1ZzGET"
   },
   "source": [
    "Now, to start our fine-tuning, we have to apply some preprocessing to the model to prepare it for training. For that use the `prepare_model_for_kbit_training` method from PEFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "a9EUEDAl0ss3"
   },
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "gkIcwsSU01EB"
   },
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "cUYEpEK-yRgj"
   },
   "source": [
    "Let's print the model to examine its layers, as we will apply QLoRA to all the linear layers of the model. Those layers are `q_proj`, `k_proj`, `v_proj`, `o_proj`, `gate_proj`, `up_proj`, `down_proj`, and `lm_head`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "XshGNsbxyRgj",
    "outputId": "c619b0e8-8516-4d4b-9abe-13eaa3f3b204",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "    (rotary_emb): MistralRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "I6mTLuQJyRgj"
   },
   "source": [
    "Here we define the LoRA config.\n",
    "\n",
    "`r` is the rank of the low-rank matrix used in the adapters, which thus controls the number of parameters trained. A higher rank will allow for more expressivity, but there is a compute tradeoff.\n",
    "\n",
    "`alpha` is the scaling factor for the learned weights. The weight matrix is scaled by `alpha/r`, and thus a higher value for `alpha` assigns more weight to the LoRA activations.\n",
    "\n",
    "The values used in the QLoRA paper were `r=64` and `lora_alpha=16`, and these are said to generalize well, but we will use `r=32` and `lora_alpha=64` so that we have more emphasis on the new fine-tuned data while also reducing computational complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Ybeyl20n3dYH",
    "outputId": "6a16c182-04d9-4812-ae81-502a8fe364d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 85041152 || all params: 3837112320 || trainable%: 2.2162799758751914\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,  # Conventional\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "X_FHi_VLyRgn"
   },
   "source": [
    "See how the model looks different now, with the LoRA adapters added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "IaYMWak4yRgn",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): MistralForCausalLM(\n",
      "      (model): MistralModel(\n",
      "        (embed_tokens): Embedding(32000, 4096)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x MistralDecoderLayer(\n",
      "            (self_attn): MistralAttention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (mlp): MistralMLP(\n",
      "              (gate_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=14336, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=14336, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=14336, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "            (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (rotary_emb): MistralRotaryEmbedding()\n",
      "      )\n",
      "      (lm_head): lora.Linear(\n",
      "        (base_layer): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "        (lora_dropout): ModuleDict(\n",
      "          (default): Dropout(p=0.05, inplace=False)\n",
      "        )\n",
      "        (lora_A): ModuleDict(\n",
      "          (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "        )\n",
      "        (lora_B): ModuleDict(\n",
      "          (default): Linear(in_features=32, out_features=32000, bias=False)\n",
      "        )\n",
      "        (lora_embedding_A): ParameterDict()\n",
      "        (lora_embedding_B): ParameterDict()\n",
      "        (lora_magnitude_vector): ModuleDict()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "_0MOtwf3zdZp"
   },
   "source": [
    "### 5. Run Training!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "fEe0uWYSyRgo"
   },
   "source": [
    "I didn't have a lot of training samples: only about 200 total train/validation. I used 500 training steps, and I was fine with overfitting in this case. I found that the end product worked well. It took about 20 minutes on the 1x A10G 24GB.\n",
    "\n",
    "Overfitting is when the validation loss goes up (bad) while the training loss goes down significantly, meaning the model is learning the training set really well, but is unable to generalize to new datapoints. In most cases, this is not desired, but since I am just playing around with a model to generate outputs like my journal entries, I was fine with a moderate amount of overfitting.\n",
    "\n",
    "With that said, a note on training: you can set the `max_steps` to be high initially, and examine at what step your model's performance starts to degrade. There is where you'll find a sweet spot for how many steps to perform. For example, say you start with 1000 steps, and find that at around 500 steps the model starts overfitting, as described above. Therefore, 500 steps would be your sweet spot, so you would use the `checkpoint-500` model repo in your output dir (`mistral-journal-finetune`) as your final model in step 6 below.\n",
    "\n",
    "If you're just doing something for fun like I did and are OK with overfitting, you can try different checkpoint versions with different degrees of overfitting.\n",
    "\n",
    "You can interrupt the process via Kernel -> Interrupt Kernel in the top nav bar once you realize you didn't need to train anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "c_L1131GyRgo"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1: # If more than 1 GPU\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "yxSbpKQSLY6B"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "from accelerate import Accelerator\n",
    "accelerator = Accelerator()\n",
    "\n",
    "model = accelerator.prepare_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "jq0nX33BmfaC",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='376' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [376/500 1:13:37 < 24:24, 0.08 it/s, Epoch 0.13/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>3.219300</td>\n",
       "      <td>2.818201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.735900</td>\n",
       "      <td>2.633218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>2.524700</td>\n",
       "      <td>2.601293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.509900</td>\n",
       "      <td>2.561702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>2.659200</td>\n",
       "      <td>2.557474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.523600</td>\n",
       "      <td>2.532127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>2.636000</td>\n",
       "      <td>2.524190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.540600</td>\n",
       "      <td>2.507073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>2.296100</td>\n",
       "      <td>2.495601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.450600</td>\n",
       "      <td>2.487588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>2.611100</td>\n",
       "      <td>2.476297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.370100</td>\n",
       "      <td>2.468562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>2.365300</td>\n",
       "      <td>2.468786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>2.441000</td>\n",
       "      <td>2.454005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='187' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 63/187 01:30 < 03:00, 0.69 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:211: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:211: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:211: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:211: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:211: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:211: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:211: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:211: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:211: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:211: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:211: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:211: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:211: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 37\u001b[0m\n\u001b[1;32m      9\u001b[0m trainer \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m     10\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     11\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtokenized_train_dataset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mtransformers\u001b[38;5;241m.\u001b[39mDataCollatorForLanguageModeling(tokenizer, mlm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m     34\u001b[0m )\n\u001b[1;32m     36\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# silence the warnings. Please re-enable for inference!\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py:2173\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2171\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2172\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2174\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2178\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py:2600\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2598\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m steps_skipped) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n\u001b[1;32m   2599\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 2600\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m\n\u001b[1;32m   2602\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2603\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2604\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py:3073\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time)\u001b[0m\n\u001b[1;32m   3071\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3072\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_evaluate:\n\u001b[0;32m-> 3073\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3074\u001b[0m     is_new_best_metric \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_determine_best_metric(metrics\u001b[38;5;241m=\u001b[39mmetrics, trial\u001b[38;5;241m=\u001b[39mtrial)\n\u001b[1;32m   3076\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_strategy \u001b[38;5;241m==\u001b[39m SaveStrategy\u001b[38;5;241m.\u001b[39mBEST:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py:3027\u001b[0m, in \u001b[0;36mTrainer._evaluate\u001b[0;34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[0m\n\u001b[1;32m   3026\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, ignore_keys_for_eval, skip_scheduler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m-> 3027\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3028\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[1;32m   3030\u001b[0m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py:4075\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4072\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   4074\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 4075\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4076\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4077\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4078\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   4079\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   4080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   4081\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4082\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4083\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4085\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   4086\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py:4269\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4266\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m observed_batch_size\n\u001b[1;32m   4268\u001b[0m \u001b[38;5;66;03m# Prediction step\u001b[39;00m\n\u001b[0;32m-> 4269\u001b[0m losses, logits, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprediction_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4270\u001b[0m main_input_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain_input_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   4271\u001b[0m inputs_decode \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   4272\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_input(inputs[main_input_name]) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m args\u001b[38;5;241m.\u001b[39minclude_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   4273\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py:4485\u001b[0m, in \u001b[0;36mTrainer.prediction_step\u001b[0;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[0m\n\u001b[1;32m   4483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_labels \u001b[38;5;129;01mor\u001b[39;00m loss_without_labels:\n\u001b[1;32m   4484\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 4485\u001b[0m         loss, outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   4486\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m   4488\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py:3733\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3731\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[1;32m   3732\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[0;32m-> 3733\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3734\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3735\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3736\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/operations.py:819\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 819\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/operations.py:807\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 807\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/autocast_mode.py:44\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/peft_model.py:1699\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1697\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1698\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1699\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1700\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1701\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1702\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1703\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1704\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1705\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1708\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1710\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m   1711\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1712\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:197\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/mistral/modeling_mistral.py:835\u001b[0m, in \u001b[0;36mMistralForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    832\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m    834\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 835\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    849\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    850\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/mistral/modeling_mistral.py:564\u001b[0m, in \u001b[0;36mMistralModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    552\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    553\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    554\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    561\u001b[0m         position_embeddings,\n\u001b[1;32m    562\u001b[0m     )\n\u001b[1;32m    563\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 564\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    576\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/mistral/modeling_mistral.py:243\u001b[0m, in \u001b[0;36mMistralDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    231\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Unpack[FlashAttentionKwargs],\n\u001b[1;32m    240\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mFloatTensor, Optional[Tuple[torch\u001b[38;5;241m.\u001b[39mFloatTensor, torch\u001b[38;5;241m.\u001b[39mFloatTensor]]]:\n\u001b[1;32m    241\u001b[0m     residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m--> 243\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_layernorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     hidden_states, self_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[1;32m    247\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m    248\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    256\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/mistral/modeling_mistral.py:213\u001b[0m, in \u001b[0;36mMistralRMSNorm.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    211\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m    212\u001b[0m variance \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 213\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m*\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrsqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvariance\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvariance_epsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m*\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mto(input_dtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from datetime import datetime\n",
    "\n",
    "project = \"journal-finetune\"\n",
    "base_model_name = \"mistral\"\n",
    "run_name = base_model_name + \"-\" + project\n",
    "output_dir = \"./\" + run_name\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        warmup_steps=2,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=1,\n",
    "        gradient_checkpointing=True,\n",
    "        max_steps=500,\n",
    "        learning_rate=2.5e-5, # Want a small lr for finetuning\n",
    "        bf16=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        logging_steps=25,              # When to start reporting loss\n",
    "        logging_dir=\"./logs\",        # Directory for storing logs\n",
    "        save_strategy=\"steps\",       # Save the model checkpoint every logging step\n",
    "        save_steps=25,                # Save checkpoints every 50 steps\n",
    "        evaluation_strategy=\"steps\", # Evaluate the model every logging step\n",
    "        eval_steps=25,               # Evaluate and save checkpoints every 50 steps\n",
    "        do_eval=True,                # Perform evaluation at the end of training\n",
    "        # report_to=\"wandb\",           # Comment this out if you don't want to use weights & baises\n",
    "        # run_name=f\"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\"          # Name of the W&B run (optional)\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
