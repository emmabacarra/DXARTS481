{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a00c4567-5baf-44dd-a67a-34cbf76f41a9",
   "metadata": {},
   "source": [
    "### Text Generation Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d018229f-cebf-479d-ab9f-d04d6aa579cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a prompt (or type 'exit' to quit):  this is my home\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: this is my home\n",
      " in the quiet and wonderful North Adams--two stories from I'll give you an easy commute if you need it, 2 stories from the Tacoma media area.\n",
      "I work from home two days a week, one out of respect for myself, and two to keep the house clean and orderly. I keep all common areas clean as a group. I expect my roommates to be respectful and conscientious, pretty quiet at times.\n",
      "Not a partying cat, but I do occasionally have a\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a prompt (or type 'exit' to quit):  exit\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('./myLM')\n",
    "model = GPT2LMHeadModel.from_pretrained('./myLM_gpt2')\n",
    "\n",
    "# Define the pad_token as eos_token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Update the model's configuration to recognize the pad_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Configure device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS backend\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "def generate_text(prompt, max_length=100):\n",
    "    # Encode the prompt and generate attention_mask\n",
    "    encoded = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors='pt',\n",
    "        padding=False,  # No padding needed for single inputs\n",
    "        truncation=True,\n",
    "        max_length=512  # Adjust based on model's max input length\n",
    "    )\n",
    "    input_ids = encoded['input_ids'].to(device)\n",
    "    attention_mask = encoded['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,  # Pass the attention_mask\n",
    "            max_length=max_length,\n",
    "            num_beams=1,\n",
    "            no_repeat_ngram_size=0,\n",
    "            early_stopping=True,\n",
    "            temperature=1.9,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id  # Ensure pad_token_id is set\n",
    "        )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "def main():\n",
    "    while True:\n",
    "        prompt = input(\"Enter a prompt (or type 'exit' to quit): \")\n",
    "        if prompt.lower() == 'exit':\n",
    "            break\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        print(generate_text(prompt)[len(prompt):], sep=\"\")\n",
    "        print(\"\\n\" + \"-\"*60 + \"\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if \"ipykernel_launcher\" in sys.argv[0]:\n",
    "        main()\n",
    "    else:\n",
    "        # Running as a standalone script\n",
    "        parser = argparse.ArgumentParser(description=\"Generate text based on a prompt.\")\n",
    "        parser.add_argument(\"--prompt\", nargs=\"?\", default=None, help=\"The prompt to generate text from.\")\n",
    "        args = parser.parse_args()\n",
    "        main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dxarts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
